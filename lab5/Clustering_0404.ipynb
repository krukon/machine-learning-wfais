{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "In this course we go through clustering methods. It is divided into few parts:\n",
    "1. Distributed clustering: \n",
    "    - K-means (HCM)\n",
    "    - Fuzzy clustering (FCM)\n",
    "    - Possibilistic clustering (PCM)\n",
    "2. Hierarhical clustering: \n",
    "    - agglomerative\n",
    "    - divisive\n",
    "3. Density-based clustering\n",
    "4. Quality metrics\n",
    "5. Image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed clustering\n",
    "\n",
    "We have three types of distributed clustering. The most known method is called k-means and assign each case to one cluster strictly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means\n",
    "\n",
    "Is also known as hard c-means where k is the same as c and are the number of clusters that we are willing to divide the data set to. The steps of hcm are like following:\n",
    "1. choose the entrance cluster centroids,\n",
    "2. item calculate the assignation matrix $U$,\n",
    "3. item calculate new centroids matrix $V$,\n",
    "4. calculate the difference between previously assignation matrix $U$ and the new one calculated in current iteration.\n",
    "\n",
    "\n",
    "Let's use the data set from the lecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Aircraft name** | **Distance range (km)** | **Seats count** | **Aircraft type** |\n",
    "|------------------|-------------------------|-----------------|-------------------|\n",
    "| Cesna 510 Mustang| 1940                    |             4   | private jet       |\n",
    "| Falcon 10/100    | 2960                    |             9   | private jet       |\n",
    "| Hawker 900/900XP | 4630                    |             9   | private jet       |\n",
    "| ATR 72-600       | 1528                    |            78   | medium size aircraft|\n",
    "| Bombardier Dash 8 Q400 | 2040              |            90   | medium size aircraft|\n",
    "| Embraer ERJ145 XR| 3700                    |            50   | medium size aircraft|\n",
    "| Boeing 747-8     | 14815                   |           467   | jet airliner      |\n",
    "| A380-800         | 15200                   |           509   | jet airliner      |\n",
    "| Boeing 787-8     | 15700                   |           290   | jet airliner      |\n",
    "| Boeing 737-900ER | 6045                    |           215   | jet airliner      | \n",
    "\n",
    "\n",
    "Let's import two libraries that are needed to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([(4,1940),(9,2960),(9,4630),(78,1528),(90,2040),(50,3700),(467,14815),(509,15200),(290,15700),(215,6045)])\n",
    "\n",
    "x1 = np.array(X[:,0])\n",
    "x2 = np.array(X[:,1])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x1,x2)\n",
    "ax.set(xlabel='Seats count', ylabel='Distance range (km)',\n",
    "       title='Aircrafts')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go to the next step, we need to normalize our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(X)\n",
    "max_values = train_data.max(0)\n",
    "\n",
    "X_norm = np.divide(train_data,max_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data is between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we should setup a few variables like the assignation matrix, number of clusters, the error margin and feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set=X_norm\n",
    "groups = 2\n",
    "space=[[0,1],[0,1]]\n",
    "\n",
    "error_margin = 0.01\n",
    "m = 2.0\n",
    "\n",
    "assignation=np.zeros((len(X),groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assignation matrix if filled with zeros as we don't have any guess for assignation yet. We can also fill it randomly with 1 and 0 for each group. The assignation matrix looks like following:\n",
    "\n",
    "\\begin{equation*}\n",
    "U=\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "It's time to generate centroid array randomly:\n",
    "\\begin{equation}\n",
    " V=[v_{1},v_{2},\\ldots,v_{c}].\n",
    "\\end{equation}\n",
    "\n",
    "We go through each group and add a random array of the feature space centroid positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "centers = []\n",
    "\n",
    "def select_centers():\n",
    "    global centers\n",
    "    global groups\n",
    "    global space\n",
    "    iter=0\n",
    "    while iter<groups:\n",
    "        centers.append((random.uniform(space[0][0],space[0][1]), \n",
    "                        random.uniform(space[1][0],space[1][1])))\n",
    "        iter=iter+1\n",
    "        \n",
    "select_centers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look what centroids do we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check what is the distance between the centroids and the elements of data set we use the Euclidean distance:\n",
    "\n",
    "\\begin{equation}\n",
    " \\rho_{Min}(x_{i},v_{j})=\\sqrt{\\sum_{i=1}^{d}(x_{i}-v_{j})^{2}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_distance(x,v):\n",
    "    return math.sqrt((x[0]-v[0])**2+(x[1]-v[1])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to calculate the new assignation matrix:\n",
    "\n",
    "\\begin{equation}\n",
    " \\mu_{ik}^{(t)}=\n",
    " \\begin{cases}\n",
    " 1 & \\text{if } d(x_{k},v_{i})<d(x_{k},v_{j}),  \\text{for each } j\\neq i\\\\\n",
    " 0 & \\text{in other case} \\\\\n",
    " \\end{cases}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_u(x, i):\n",
    "    global centers\n",
    "    if calculate_distance(x, centers[0]) < calculate_distance(x, centers[1]):\n",
    "        return [1,0]\n",
    "    else:\n",
    "        return [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to calculate new centroids based on the new assignation matrix $U$:\n",
    "\n",
    "\\begin{equation}\n",
    " v_{i}=\\frac{\\sum_{k=1}^{M}\\mu_{ik}^{(t)}x_{k}}{\\sum_{k=1}^{M}\\mu_{ik}^{(t)}}.\n",
    "\\end{equation}\n",
    "\n",
    "The calculation is done in two steps: u_x_vector and u_scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_new_centers(u):\n",
    "    global centers\n",
    "    new_centers=[]\n",
    "    for c in range(groups):\n",
    "        u_x_vector=np.zeros(2)\n",
    "        u_scalar=0.0\n",
    "        for i in range(len(data_set)):\n",
    "            u_scalar = u_scalar+(u[i][c]**m)\n",
    "            u_x_vector=np.add(u_x_vector,np.multiply(u[i][c]**m,data_set[i]))\n",
    "        new_centers.append(np.divide(u_x_vector,u_scalar))\n",
    "    centers=new_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost done here. The last step before we cluster is to set the rule that allow us to stop the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_differences(new_assignation):\n",
    "    global assignation    \n",
    "    return np.sum(np.abs(np.subtract(assignation,new_assignation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to combine all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster():\n",
    "    global assignation    \n",
    "    global error_margin\n",
    "    difference_limit_not_achieved=True\n",
    "    iter=0\n",
    "    while difference_limit_not_achieved:\n",
    "        new_assignation=[]\n",
    "        for i in range(len(data_set)):\n",
    "            new_assignation.append(calculate_u(data_set[i], iter))\n",
    "        calculate_new_centers(new_assignation)\n",
    "        if iter>0:\n",
    "            if calculate_differences(new_assignation) < error_margin:\n",
    "                difference_limit_not_achieved=False\n",
    "        assignation=new_assignation\n",
    "        iter=iter+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to build some new clusters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centers are like following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And the assignation matrix looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assignation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot it, we need to develop a short function that adds some colors to our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = X_norm[np.where(np.array(assignation)[:,0]==1)]\n",
    "blue = X_norm[np.where(np.array(assignation)[:,1]==1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(blue[:,0],blue[:,1],c='blue')\n",
    "ax.scatter(red[:,0],red[:,1],c='red')\n",
    "ax.scatter(np.array(centers)[:,0],np.array(centers)[:,1],c='black')\n",
    "ax.set(xlabel='Seats count', ylabel='Distance range (km)',\n",
    "       title='Aircrafts (clusters)')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Modify the code to work for three groups (max. 15min.)\n",
    "\n",
    "The obvious part is the variable groups, but the most changes needs to be done here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = 3\n",
    "\n",
    "select_centers()\n",
    "\n",
    "def calculate_u_three(x):\n",
    "    global centers\n",
    "    global groups\n",
    "    u_array = np.zeros(groups)\n",
    "    minimal_distance = []\n",
    "    for group in range(groups):\n",
    "        minimal_distance.append(calculate_distance(x, centers[group]))\n",
    "    min_group_id = np.argmin(minimal_distance)\n",
    "    u_array[min_group_id] = 1\n",
    "    return u_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster():\n",
    "    global assignation    \n",
    "    global error_margin\n",
    "    difference_limit_not_achieved=True\n",
    "    iter=0\n",
    "    while difference_limit_not_achieved:\n",
    "        new_assignation=[]\n",
    "        for i in range(len(data_set)):\n",
    "            new_assignation.append(calculate_u_three(data_set[i]))\n",
    "        calculate_new_centers(new_assignation)\n",
    "        if iter>0:\n",
    "            if calculate_differences(new_assignation) < error_margin:\n",
    "                difference_limit_not_achieved=False\n",
    "        assignation=new_assignation\n",
    "        iter=iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals:\n",
    "1. Modify the calculate_u code.\n",
    "2. Modify the parameters.\n",
    "3. Execute the clustering.\n",
    "4. Plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = X_norm[np.where(np.array(assignation)[:,0]==1)]\n",
    "blue = X_norm[np.where(np.array(assignation)[:,1]==1)]\n",
    "green = X_norm[np.where(np.array(assignation)[:,2]==1)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(blue[:,0],blue[:,1],c='blue')\n",
    "ax.scatter(red[:,0],red[:,1],c='red')\n",
    "ax.scatter(green[:,0],green[:,1],c='green')\n",
    "ax.scatter(np.array(centers)[:,0],np.array(centers)[:,1],marker='x',c='black')\n",
    "ax.set(xlabel='Seats count', ylabel='Distance range (km)',\n",
    "       title='Aircrafts (clusters)')\n",
    "ax.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy k-means\n",
    "\n",
    "The fuzzy implementation of k-means is a bit more complex and we need to modify the calculate_u function to be complient with the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " \\mu_{ik}=(\\sum_{j=1}^{c}(\\frac{d(x_{k},v_{i})}{d(x_{k},v_{j})})^{\\frac{2}{m-1}})^{-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_u(x,i):\n",
    "    global centers\n",
    "    if i == 0:\n",
    "        sum=1.0+(calculate_distance(x, centers[0])/calculate_distance(x, centers[1]))**2\n",
    "    else:\n",
    "        sum=1.0+(calculate_distance(x, centers[1])/calculate_distance(x, centers[0]))**2\n",
    "    return sum**-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = 2 \n",
    "assignation=np.zeros((len(X),groups))\n",
    "centers=[]\n",
    "select_centers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centers)\n",
    "#centers = [(0.5,0.6),(0.6,0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster():\n",
    "    global assignation    \n",
    "    global error_margin    \n",
    "    global groups\n",
    "    difference_limit_not_achieved=True\n",
    "    iter=0\n",
    "    while difference_limit_not_achieved:\n",
    "        new_assignation=[]\n",
    "        for i in range(len(data_set)):\n",
    "            new_assignation_vector=[]\n",
    "            for k in range(groups):\n",
    "                new_assignation_vector.append(calculate_u(data_set[i],k))\n",
    "            new_assignation.append(new_assignation_vector)\n",
    "        calculate_new_centers(new_assignation)\n",
    "\n",
    "        if iter>0:\n",
    "            if calculate_differences(new_assignation) < error_margin:\n",
    "                difference_limit_not_achieved=False\n",
    "        assignation=new_assignation\n",
    "        iter=iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assignation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = X_norm[np.where(np.array(assignation)[:,0]>0.5)]\n",
    "blue = X_norm[np.where(np.array(assignation)[:,1]>0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(blue[:,0],blue[:,1],c='blue')\n",
    "ax.scatter(red[:,0],red[:,1],c='red')\n",
    "ax.scatter(np.array(centers)[:,0],np.array(centers)[:,1],c='black')\n",
    "ax.set(xlabel='Seats count', ylabel='Distance range (km)',\n",
    "       title='Aircrafts (clusters)')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework: Implement possibilistic k-means\n",
    "\n",
    "Goal:\n",
    "1. Implement the mahalanobis_distance function.\n",
    "2. Implement the calculate_eta function.\n",
    "3. Implement the calculate_u.\n",
    "\n",
    "Hint: the assignation matrix should not be set to zeros at the beginning. The equations can be found in the slides. \n",
    "\n",
    "**Deadline:** 4.04.2018\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-based clustering\n",
    "\n",
    "In density-based clustering the approach is different compared to distributed clustering. We need to implement all functions from scratch. DBScan is an example of a density-based clustering method. The goal is to find all element where the neighborhood is defined as:\n",
    "\\begin{equation}\n",
    "    N_{\\epsilon}:{q|d(p,q)\\leq\\epsilon},\n",
    "\\end{equation}\n",
    "where $p$ and $q$ are two elements of the training data set and $\\epsilon$ is the neighborhood distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the variables as in previous examples. The are three new ones like distance_matrix, max_distance and number_of_cluster. The first one is clear, the second is a parameter that can be changed, depending on that how many neighborhood elements we would like to concider. The last variable is about the number of clusters that are calculated during clustering. It's not the exact number of clusters, but allow us count the clusters during clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = X_norm\n",
    "assignation = np.zeros(len(data_set))\n",
    "distance_matrix = np.zeros((len(data_set), len(data_set)))\n",
    "max_distance = 0.35\n",
    "number_of_cluster = 0\n",
    "min_points = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the distance matrix we use the calculate_distance that we used previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_distance_matrix():\n",
    "    global distance_matrix\n",
    "    for i in range(len(data_set)):\n",
    "        for j in range(len(data_set)):\n",
    "            distance_matrix[i, j] = calculate_distance(data_set[i], data_set[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to get closest elements in the feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_closest_elements(element_id):\n",
    "    global max_distance\n",
    "    global distance_matrix\n",
    "    element_distances = distance_matrix[element_id]\n",
    "    filtered = {}\n",
    "    iter = 0\n",
    "    for element in element_distances:\n",
    "        if element < max_distance:\n",
    "            filtered[iter] = element\n",
    "        iter = iter + 1\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before cluster function is to define funtions that mark the elements in our data set that are known to be a noise or were already visited by our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_as_noise(element_id):\n",
    "    global assignation\n",
    "    assignation[element_id] = -1\n",
    "    \n",
    "def set_visited(elements):\n",
    "    global assignation    \n",
    "    global number_of_cluster\n",
    "    for element_id in elements.keys():\n",
    "        assignation[element_id] = number_of_cluster    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def cluster():\n",
    "    global number_of_cluster\n",
    "    global data_set\n",
    "    global assignation\n",
    "    global min_points\n",
    "    calculate_distance_matrix()\n",
    "    element_ids = list(range(len(data_set)))\n",
    "    random.shuffle(element_ids)\n",
    "    for i in element_ids:\n",
    "        if assignation[i] != 0:\n",
    "            continue\n",
    "        closest = get_closest_elements(i)\n",
    "        if len(closest) < min_points:\n",
    "            set_as_noise(i)\n",
    "        else:\n",
    "            set_visited(closest)\n",
    "            number_of_cluster = number_of_cluster + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of cluster is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of clusters: \"+ str(len(np.unique(assignation))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it here as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assignation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Plot the feature space with all element marked with differnet color, depending on the cluster that it's assigned\n",
    "\n",
    "Use the code below to plot the results. You can play with the max_distance variable to get more or less groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assignation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k-means:\n",
    "assigned_groups = []\n",
    "colors = ['red','blue','green','orange','black','yellow']\n",
    "\n",
    "for el in range(len(X_norm)):\n",
    "    group_id = np.argmax(assignation[el])\n",
    "    assigned_groups.append(group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colours(color_id):\n",
    "    global X_norm\n",
    "    print(color_id)\n",
    "    print(X_norm[np.where(np.array(assigned_groups)[:]==color_id)])\n",
    "    return X_norm[np.where(np.array(assigned_groups)[:]==color_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red','blue','green','orange','black','yellow']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# your code or modifications comes here:\n",
    "# for dbscan:\n",
    "assigned_groups = assignation\n",
    "print(assgined_groups)\n",
    "#for group in range(groups):\n",
    "for group in np.unique(assigned_groups):\n",
    "    small_set = get_colours(group)    \n",
    "    ax.scatter(small_set[:,0],small_set[:,1],c=colors.pop(0))\n",
    "# k-means:\n",
    "#ax.scatter(np.array(centers)[:,0],np.array(centers)[:,1],marker='x',c='black')\n",
    "# ends here\n",
    "ax.set(xlabel='Seats count', ylabel='Distance range (km)',\n",
    "       title='Aircrafts (clusters)')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hierchical methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two different types of hierarchical methods:\n",
    "- agglomerative,\n",
    "- divisive.\n",
    "\n",
    "Both got the same concept:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pydot enables to use graphviz to visualize a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to keep the dendrogram and the history, so we can easily draw it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = X_norm\n",
    "distance_matrix = np.zeros((len(data_set), len(data_set)))\n",
    "current_dendrograms = []\n",
    " \n",
    "for iter in range(len(data_set)):\n",
    "    current_dendrograms.append(data_set[iter])\n",
    "dendrograms_history = []\n",
    "\n",
    "dendrograms_hist = []\n",
    "for iter in [range(len(data_set))]:\n",
    "    dendrograms_hist.append(iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree object is the root where each edge is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = pydot.Dot(graph_type='graph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Euclidean distance was already used, but this time we can encapsulate it into a class, to be clear it's Euclidean distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EuclidesianDistance:\n",
    "\n",
    "    def get_distance(self, x1, x2):\n",
    "        return math.sqrt((x1[0]-x2[0])**2+(x1[1]-x2[1])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In agglomerative we merge two clusters together on each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_elements(merged_list, i):\n",
    "    global current_dendrograms\n",
    "    if isinstance(current_dendrograms[i][0], type(np.array([]))):\n",
    "        for iter in range(len(current_dendrograms[i])):\n",
    "            merged_list.append(current_dendrograms[i][iter])\n",
    "    else:\n",
    "        merged_list.append(current_dendrograms[i])\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "We need to set the current dendrogram level at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_current_dendrogram(i, j):\n",
    "    global dendrograms_hist\n",
    "    global current_dendrograms\n",
    "    elements = []\n",
    "    hist = []\n",
    "    current_hist = dendrograms_hist[-1]\n",
    "    for iter in range(len(current_dendrograms)):\n",
    "        if iter != i and iter !=j:\n",
    "            elements.append(current_dendrograms[iter])\n",
    "            hist.append(current_hist[iter])\n",
    "    merged_elements = []\n",
    "    merged_elements = merge_elements(merged_elements, i)\n",
    "    merged_elements = merge_elements(merged_elements, j)\n",
    "    elements.append(merged_elements)\n",
    "    hist.append([current_hist[i],current_hist[j]])\n",
    "    dendrograms_hist.append(hist)\n",
    "    current_dendrograms = elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each stage a distance matrix needs to be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_dendogram_distance_matrix():\n",
    "    global current_dendrograms\n",
    "    global distance_matrix\n",
    "    distance_matrix = np.zeros((len(current_dendrograms), len(current_dendrograms)))\n",
    "    euclidean = EuclidesianDistance()\n",
    "    for i in range(len(current_dendrograms)):\n",
    "        for j in range(len(current_dendrograms)):\n",
    "            distance_matrix[i, j] = euclidean.get_distance(calculate_centroid(current_dendrograms[i]),calculate_centroid(current_dendrograms[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code abobve needs the centroids' average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_centroid(dendrogram_elements):\n",
    "    global data_set\n",
    "    if type(dendrogram_elements) is list:\n",
    "        sum=np.zeros(len(dendrogram_elements[0]))\n",
    "        for iter in range(len(dendrogram_elements)):\n",
    "            sum=np.add(sum,np.array(dendrogram_elements[iter]))\n",
    "        if sum.shape == (len(data_set[0]),len(data_set[0])):\n",
    "            pass\n",
    "        return np.divide(sum*1.0, len(dendrogram_elements)*1.0)\n",
    "    else:\n",
    "        return dendrogram_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For agglomerative we take the lowest distance from the distance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lowest_from_distance_matrix():\n",
    "    global distance_matrix\n",
    "    lowest_indexes = [0,1]\n",
    "    lowest_value = distance_matrix[0,1]\n",
    "    for i in range(len(distance_matrix)):\n",
    "        for j in range(len(distance_matrix)):\n",
    "            if i != j:\n",
    "                if lowest_value > distance_matrix[i,j]:\n",
    "                    lowest_value = distance_matrix[i,j]\n",
    "                    lowest_indexes = [i,j]\n",
    "    return lowest_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine it together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster():\n",
    "    global current_dendrograms\n",
    "    for iter in range(len(current_dendrograms)-2):\n",
    "        calculate_dendogram_distance_matrix()\n",
    "        [i, j] = get_lowest_from_distance_matrix()\n",
    "        dendrograms_history.append(current_dendrograms)\n",
    "        set_current_dendrogram(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And execute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dendrograms_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pydot to print the dendrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edge(level, acesor):\n",
    "    global tree\n",
    "    new_edge = pydot.Edge(str(acesor), str(level))\n",
    "    tree.add_edge(new_edge)\n",
    "    if isinstance(level,list):\n",
    "        add_edge(level[0],level)\n",
    "        add_edge(level[1],level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_tree():\n",
    "    global dendrograms_hist\n",
    "    global tree\n",
    "    current_list = dendrograms_hist.pop()\n",
    "    add_edge(current_list.pop(),\"root\")\n",
    "    tree.write('tree_agg.png',format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at tree_agg.png if generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisive clustering\n",
    "\n",
    "Compared to the previous methods, we do the same, but in the opposite direction. We setup the same data, but the current dendrogram contains the whole data set as one cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = X_norm\n",
    "distance_matrix = np.zeros((len(data_set), len(data_set)))\n",
    "current_dendrograms = []\n",
    "current_dendrograms.append(data_set)\n",
    "dendrograms_history = []\n",
    "clusters = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same distance matrix methods as used for agglomerative method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_dendogram_distance_matrix():\n",
    "    global distance_matrix\n",
    "    distance_matrix=np.zeros((len(data_set), len(data_set)))\n",
    "    euclidean = EuclidesianDistance()\n",
    "    for i in range(len(data_set)):\n",
    "        for j in range(len(data_set)):\n",
    "            distance_matrix[i, j] = euclidean.get_distance(calculate_centroid(data_set[i]),calculate_centroid(data_set[j]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the highest average distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_highest_dissimilarity_average(cluster):\n",
    "    highest_distance = np.average(np.array(distance_matrix[cluster[0]]))\n",
    "    highest_iter = cluster[0]\n",
    "    for iter in cluster:\n",
    "        average_distance = np.average(np.array(distance_matrix[iter]))\n",
    "        if highest_distance < average_distance:\n",
    "            highest_iter=iter\n",
    "            highest_distance=average_distance\n",
    "    return highest_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_negative(cluster_id, element_id):\n",
    "    global distance_matrix\n",
    "    global current_dendrograms\n",
    "    is_all_negative = True\n",
    "    new_matrix = calculate_new_dist_matrix(cluster_id)\n",
    "    for iter in range(len(new_matrix)):\n",
    "        if (np.average(np.array(new_matrix[iter])) - distance_matrix[current_dendrograms[cluster_id][iter], element_id]) > 0:\n",
    "            is_all_negative = False\n",
    "    return is_all_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_new_dist_matrix(cluster_id):\n",
    "    global distance_matrix\n",
    "    global current_dendrograms\n",
    "    global data_set\n",
    "    matrix = np.matrix(distance_matrix)\n",
    "    ids = current_dendrograms[cluster_id]\n",
    "    for iter in list(range(len(data_set)-1,-1,-1)):\n",
    "        if iter not in ids:\n",
    "            matrix = np.delete(matrix, iter, 0)\n",
    "            matrix = np.delete(matrix, iter, 1)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_highest_positive(cluster_id, element_id):\n",
    "    global current_dendrograms\n",
    "    current_cluster = current_dendrograms[cluster_id]\n",
    "    dist_matrix = calculate_new_dist_matrix(cluster_id)\n",
    "    max = np.average(current_cluster[0])\n",
    "    max_id = 0\n",
    "    for iter in range(len(current_cluster)):\n",
    "            if (np.average(np.array(dist_matrix[iter])) - distance_matrix[current_dendrograms[cluster_id][iter], element_id]) > max:\n",
    "                max = (np.average(np.array(dist_matrix[iter])) - distance_matrix[current_dendrograms[cluster_id][iter], element_id])\n",
    "                max_id = iter\n",
    "    return current_dendrograms[cluster_id][max_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diam(cluster_id):\n",
    "    dist_matrix = calculate_new_dist_matrix(cluster_id)\n",
    "    return np.matrix(dist_matrix).max()\n",
    "\n",
    "def choose_cluster():\n",
    "    max = get_diam(0)\n",
    "    max_id = 0\n",
    "    for iter in range(len(current_dendrograms)):\n",
    "        if max < get_diam(iter):\n",
    "            max_id = iter\n",
    "    return max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Combine it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def cluster():\n",
    "    global current_dendrograms\n",
    "    global data_set\n",
    "    current_dendrograms=[list(range(len(data_set)))]\n",
    "    while len(current_dendrograms) != len(data_set):\n",
    "        current = deepcopy(current_dendrograms)\n",
    "        split_cluster = []\n",
    "        cluster_id = choose_cluster()\n",
    "        highest_id = get_highest_dissimilarity_average(current_dendrograms[cluster_id])\n",
    "        split_cluster.append(highest_id)\n",
    "        current_dendrograms[cluster_id].remove(highest_id)\n",
    "        all_positive = True\n",
    "        while all_positive:\n",
    "            if all_negative(cluster_id, highest_id):\n",
    "                all_positive = False\n",
    "            else:\n",
    "                highest_diff_id = get_highest_positive(cluster_id, highest_id) # do poprawki\n",
    "                split_cluster.append(highest_diff_id)\n",
    "                current_dendrograms[cluster_id].remove(highest_diff_id)\n",
    "        level = [item for item in current[cluster_id] if item not in split_cluster]\n",
    "        hist = [{\"acesor\": current[cluster_id] , \"levels\": [deepcopy(split_cluster), level]}]\n",
    "        dendrograms_history.append(hist)\n",
    "        current_dendrograms.append(split_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = pydot.Dot(graph_type='graph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Build a decision tree graph using dendrograms_history and pydot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_s_1(centers,assignation,data_set):\n",
    "    s1 = []\n",
    "    for center_1 in range(len(centers)):\n",
    "        for center_2 in range(len(centers)):\n",
    "            if center_1 == center_2:\n",
    "                break\n",
    "            ids_1 = np.where(assignation[:, center_1] == 1)[0]\n",
    "            ids_2 = np.where(assignation[:, center_2] == 1)[0]\n",
    "            elements_1 = data_set[ids_1]\n",
    "            elements_2 = data_set[ids_2]\n",
    "            s_1 = 1.0 / (len(ids_1) * len(ids_2))\n",
    "            for element_1 in elements_1:\n",
    "                for element_2 in elements_2:\n",
    "                    s_1 = s_1 * math.sqrt(calculate_distance(element_1, element_2) ** 2)\n",
    "            s1.append(s_1)\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_s_2(centers):\n",
    "    s2 = []\n",
    "    for center_1 in range(len(centers)):\n",
    "        for center_2 in range(len(centers)):\n",
    "            if center_1 == center_2:\n",
    "                break\n",
    "            s2.append(calculate_distance(centers[center_1], centers[center_2]))\n",
    "    return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_s_s_1(s1, sigma_1):\n",
    "    s_1_sum = 0.0\n",
    "    sigma_1_sum = 0.0\n",
    "    for s_1 in s1:\n",
    "        s_1_sum = s_1_sum + s_1\n",
    "    for sigma_1 in sigma_1:\n",
    "        sigma_1_sum = sigma_1_sum + sigma_1\n",
    "    s_s1 = s_1_sum / sigma_1_sum\n",
    "    return s_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sigma_1(assignation,data_set):\n",
    "    sigma_1 = []\n",
    "    unique_labels = np.unique(assignation[0])\n",
    "    for label_id in range(len(unique_labels)):\n",
    "        ids = np.where(assignation[:, label_id] == 1)[0]\n",
    "        if len(ids) == 1:\n",
    "            m = 1\n",
    "        else:\n",
    "            m = (len(ids) - 1.0) * len(ids) / 2.0\n",
    "        elements = data_set[ids]\n",
    "        sigma = (1.0 / m)\n",
    "        for element_x_1 in range(len(elements)):\n",
    "            for element_x_2 in range(len(elements)):\n",
    "                if element_x_1 == element_x_2:\n",
    "                    continue\n",
    "                distance = calculate_distance(elements[element_x_1], elements[element_x_2])\n",
    "                if distance != 0:\n",
    "                    sigma = sigma + (distance ** 2)\n",
    "        sigma_1.append(sigma)\n",
    "    return sigma_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sigma_2(centers,assignation,data_set):\n",
    "    sigma_2 = []\n",
    "    for center_id in range(len(centers)):\n",
    "        ids = np.where(assignation[:, center_id] == 1)[0]\n",
    "        elements = data_set[ids]\n",
    "        sigma = 1.0 / len(ids)\n",
    "        for element_id in range(len(elements)):\n",
    "            distance = calculate_distance(elements[element_id], centers[center_id])\n",
    "            if distance != 0:\n",
    "                sigma = sigma + (distance) ** 2\n",
    "        sigma_2.append(sigma)\n",
    "    return sigma_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dunn_index(assignation,data_set):\n",
    "    minimum_distance = 1\n",
    "    maximum_distance = 0\n",
    "    unique_labels = np.unique(assignation[0])\n",
    "    for label_id_1 in range(len(unique_labels)):\n",
    "        ids_1 = np.where(assignation[:, label_id_1] == 1)[0]\n",
    "        for label_id_2 in range(len(unique_labels)):\n",
    "            if label_id_1 == label_id_2:\n",
    "                break\n",
    "            ids_2 = np.where(assignation[:, label_id_2] == 1)[0]\n",
    "            for element_1 in data_set[ids_1]:\n",
    "                for element_2 in data_set[ids_2]:\n",
    "                    distance = calculate_distance(element_1, element_2)\n",
    "                    if distance > maximum_distance:\n",
    "                        maximum_distance = distance\n",
    "                    if distance < minimum_distance:\n",
    "                        minimum_distance = distance\n",
    "    dunn_index = minimum_distance / maximum_distance\n",
    "    return dunn_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.array([(4,1940),(9,2960),(9,4630),(78,1528),(90,2040),(50,3700),(467,14815),(509,15200),(290,15700),(215,6045)])\n",
    "\n",
    "x1 = np.array(X[:,0])\n",
    "x2 = np.array(X[:,1])\n",
    "\n",
    "train_data = np.array(X)\n",
    "max_values = train_data.max(0)\n",
    "\n",
    "X_norm = np.divide(train_data,max_values)\n",
    "\n",
    "space=[[0,1],[0,1]]\n",
    "error_margin = 0.01\n",
    "centers = []\n",
    "m = 2.0\n",
    "\n",
    "data_set=X_norm\n",
    "\n",
    "\n",
    "def select_centers():\n",
    "    global centers\n",
    "    global groups\n",
    "    global space\n",
    "    iter=0\n",
    "    while iter<groups:\n",
    "        centers.append((random.uniform(space[0][0],space[0][1]), \n",
    "                        random.uniform(space[1][0],space[1][1])))\n",
    "        iter=iter+1\n",
    "        \n",
    "select_centers()\n",
    "\n",
    "def calculate_new_centers(u):\n",
    "    global centers\n",
    "    new_centers=[]\n",
    "    for c in range(groups):\n",
    "        u_x_vector=np.zeros(2)\n",
    "        u_scalar=0.0\n",
    "        for i in range(len(data_set)):\n",
    "            u_scalar = u_scalar+(u[i][c]**m)\n",
    "            u_x_vector=np.add(u_x_vector,np.multiply(u[i][c]**m,data_set[i]))\n",
    "        new_centers.append(np.divide(u_x_vector,u_scalar))\n",
    "    centers=new_centers\n",
    "    \n",
    "def calculate_u_three(x):\n",
    "    global centers\n",
    "    global groups\n",
    "    u_array = np.zeros(groups)\n",
    "    minimal_distance = []\n",
    "    for group in range(groups):\n",
    "        minimal_distance.append(calculate_distance(x, centers[group]))\n",
    "    min_group_id = np.argmin(minimal_distance)\n",
    "    u_array[min_group_id] = 1\n",
    "    return u_array    \n",
    "\n",
    "def cluster():\n",
    "    global assignation    \n",
    "    global error_margin\n",
    "    global data_set\n",
    "    difference_limit_not_achieved=True\n",
    "    iter=0\n",
    "    while difference_limit_not_achieved:\n",
    "        new_assignation=[]\n",
    "        for i in range(len(data_set)):\n",
    "            new_assignation.append(calculate_u_three(data_set[i]))\n",
    "        calculate_new_centers(new_assignation)\n",
    "        if iter>0:\n",
    "            if calculate_differences(new_assignation) < error_margin:\n",
    "                difference_limit_not_achieved=False\n",
    "        assignation=new_assignation\n",
    "        iter=iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = 2\n",
    "assignation=np.zeros((len(X),groups))\n",
    "cluster()\n",
    "\n",
    "s1_2 = calculate_s_1(centers,np.array(assignation),data_set)\n",
    "sigma1_2 = calculate_sigma_1(np.array(assignation),data_set)\n",
    "s2_2 = calculate_s_2(centers)\n",
    "s_s_1_2 = calculate_s_s_1(s1_2,sigma1_2)\n",
    "sigma2_2 = calculate_sigma_2(centers,np.array(assignation),data_set)\n",
    "\n",
    "dunn_2 = dunn_index(np.array(assignation),data_set)\n",
    "\n",
    "print(s1_2)\n",
    "print(sigma1_2)\n",
    "print(s_s_1_2)\n",
    "print(sigma2_2)\n",
    "print(dunn_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = []\n",
    "select_centers()\n",
    "groups = 3\n",
    "m=2\n",
    "assignation=np.zeros((len(X),groups))\n",
    "cluster()\n",
    "\n",
    "s1_3 = calculate_s_1(centers,np.array(assignation),data_set)\n",
    "s2_3 = calculate_s_2(centers)\n",
    "sigma2_3 = calculate_sigma_2(centers,np.array(assignation),data_set)\n",
    "dunn_3 = dunn_index(np.array(assignation),data_set)\n",
    "sigma1_3 = calculate_sigma_1(np.array(assignation),data_set)\n",
    "\n",
    "s_s_1_3 = calculate_s_s_1(s1_3,sigma1_3)\n",
    "\n",
    "print(s1_3)\n",
    "print(s2_3)\n",
    "print(s_s_1_3)\n",
    "print(sigma2_3)\n",
    "print(dunn_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ImageConversion:\n",
    "\n",
    "    def get_image_from_url(self, img_url):\n",
    "        image = open(img_url,'rb')\n",
    "        return img.imread(image)\n",
    "\n",
    "    def get_unique_colours(self, image_matrix):\n",
    "        feature_matrix = []\n",
    "        for i in range(len(image_matrix)):\n",
    "            for j in range(len(image_matrix[0])):\n",
    "                feature_matrix.append(image_matrix[i, j])\n",
    "        feature_matrix_np = numpy.array(feature_matrix)\n",
    "        uniques, index = numpy.unique([str(i) for i in feature_matrix_np], return_index=True)\n",
    "        return feature_matrix_np[index], feature_matrix\n",
    "\n",
    "    def save_image(self, size, pixel_matrix, unique_matrix, assignation_matrix, colours, output):\n",
    "        image_out = Image.new(\"RGB\", size)\n",
    "        pixels = []\n",
    "        for i in range(len(pixel_matrix)):\n",
    "            pixel_list = pixel_matrix[i].tolist()\n",
    "            for j in range(len(unique_matrix)):\n",
    "                if(pixel_list == unique_matrix[j].tolist()):\n",
    "                    for k in range(len(colours)):\n",
    "                        if assignation_matrix[j][k] == 1:\n",
    "                            segmented_colours=[int(i) for i in (colours[k]*255)]\n",
    "                            pixels.append(tuple(segmented_colours))\n",
    "        image_out.putdata(pixels)\n",
    "        image_out.save(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Segmentation:\n",
    "\n",
    "    def __init__(self, feature_matrix, groups):\n",
    "        self.__data_set = feature_matrix\n",
    "        self.__groups = groups\n",
    "        self.__space=[[0, 255], [0, 255], [0, 255]]\n",
    "        self.__error_margin = 0.5\n",
    "        self.assignation = numpy.zeros((len(self.__data_set), self.__groups))\n",
    "        self.centers = []\n",
    "        self.select_centers()\n",
    "\n",
    "    def select_centers(self):\n",
    "        if len(self.centers) == 0:\n",
    "            iter=0\n",
    "            while iter<self.__groups:\n",
    "                self.centers.append(((random.randrange(0, 255)*1.0/255),\n",
    "                                     (random.randrange(0, 255)*1.0/255),\n",
    "                                     (random.randrange(0, 255)*1.0/255)))\n",
    "                iter=iter+1\n",
    "\n",
    "    def calculate_distance(self, x, v):\n",
    "        return math.sqrt((x[0]-v[0])**2+(x[1]-v[1])**2+(x[2]-v[2])**2)\n",
    "\n",
    "    def calculate_u(self, x, i):\n",
    "        smallest_distance = float(self.calculate_distance(x, self.centers[0]))\n",
    "        smallest_id = 0\n",
    "        for i in range(1, self.__groups):\n",
    "            distance = self.calculate_distance(x, self.centers[i])\n",
    "            if distance < smallest_distance:\n",
    "                smallest_id = i\n",
    "                smallest_distance = distance\n",
    "        distance = numpy.zeros(self.__groups)\n",
    "        distance[smallest_id]=1\n",
    "        return distance\n",
    "\n",
    "    def calculate_new_centers(self, u):\n",
    "        new_centers=[]\n",
    "        for c in range(self.__groups):\n",
    "            u_x_vector = numpy.zeros(len(self.centers[0]))\n",
    "            u_scalar = 0\n",
    "            for i in range(len(u)):\n",
    "                u_scalar = u_scalar + u[i][c]\n",
    "                u_x_vector = numpy.add(u_x_vector, numpy.multiply(u[i][c], self.__data_set[i]))\n",
    "            new_centers.append(numpy.divide(u_x_vector,u_scalar))\n",
    "        self.centers = new_centers\n",
    "\n",
    "    def calculate_differences(self,new_assignation):\n",
    "        diff=0\n",
    "        for i in range(len(self.assignation)):\n",
    "            for j in range(self.__groups):\n",
    "                diff = diff + abs(float(new_assignation[i][j]) - float(self.assignation[i][j]))\n",
    "        return diff\n",
    "\n",
    "    def do_segmentation(self):\n",
    "        difference_limit_not_achieved = True\n",
    "        iter = 0\n",
    "        while difference_limit_not_achieved:\n",
    "            new_assignation = []\n",
    "            for i in range(len(self.__data_set)):\n",
    "                new_assignation.append(self.calculate_u(self.__data_set[i], iter))\n",
    "            self.calculate_new_centers(new_assignation)\n",
    "\n",
    "            if iter > 0:\n",
    "                if self.calculate_differences(new_assignation) < self.__error_margin:\n",
    "                    difference_limit_not_achieved=False\n",
    "            self.assignation = new_assignation\n",
    "            iter = iter + 1\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.centers, self.assignation\n",
    "\n",
    "    def print_results(self):\n",
    "        print(self.assignation)\n",
    "        print(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "import math\n",
    "import matplotlib.image as img\n",
    "from PIL import Image\n",
    "\n",
    "image_to_segment = \"logo_krakow.png\"\n",
    "image_converter = ImageConversion()\n",
    "image_data = image_converter.get_image_from_url(image_to_segment)\n",
    "unique_image_data, image_data_list = image_converter.get_unique_colours(image_data)\n",
    "\n",
    "groups = 3\n",
    "\n",
    "segmentation = Segmentation(unique_image_data, groups)\n",
    "segmentation.do_segmentation()\n",
    "#segmentation.print_results()\n",
    "centers, assignation_matrix = segmentation.get_results()\n",
    "\n",
    "image_size = (232, 258)\n",
    "image_converter.save_image(image_size, image_data_list, unique_image_data, assignation_matrix, centers, \"output.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework #2\n",
    "\n",
    "Implement the Czekanowski-Dice index. \n",
    "\n",
    "\\begin{equation}\n",
    "C=2\\frac{P\\times R}{R+R}.\n",
    "\\end{equation}\n",
    "Precision is defined as:\n",
    "\\begin{equation}\n",
    "PPV=\\frac{\\#TP}{\\#TP+\\#FP}.\n",
    "\\end{equation}\n",
    "Recall is defined as:\n",
    "\\begin{equation}\n",
    "TPR=\\frac{\\#TP}{\\#TP+\\#FN}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def czekanowski_dice_index(data_set,assignation,labels):\n",
    "    assigned_groups = []\n",
    "    for el in range(len(assignation)):\n",
    "        group_id = np.argmax(assignation[el])\n",
    "        assigned_groups.append(group_id)\n",
    "    # your code goes here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
